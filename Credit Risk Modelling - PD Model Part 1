import numpy as np
import pandas as pd

loan_data_backup = pd.read_csv('C:/Users/vivek.goel/Downloads/Credit_Risk_Modelling_Udemy_course/loan_data_2007_2014.csv') 

loan_data = loan_data_backup.copy()

loan_data

#to display all columns of data
pd.options.display.max_columns = None

loan_data

loan_data.head()

loan_data.tail()

loan_data.columns.values

loan_data.info()

#Pandas call text string as object type.
#Term and emp_length are object but should be numeric

#to get unique values
loan_data['emp_length'].unique()

#we need to get rid of (years, <, +) words from the column to make it numeric
loan_data['emp_length_int'] = loan_data['emp_length'].str.replace('\+ years', '')


loan_data['emp_length_int'].unique()

loan_data['emp_length_int'] = loan_data['emp_length_int'].str.replace('< 1 year', str(0))
loan_data['emp_length_int'] = loan_data['emp_length_int'].str.replace(' years', '')
loan_data['emp_length_int'] = loan_data['emp_length_int'].str.replace(' year', '')
loan_data['emp_length_int'].unique()

#counting null strings in a column
loan_data['emp_length_int'].isnull().sum()

#replacing null values by string 0
loan_data['emp_length_int'] = loan_data['emp_length_int'].fillna("0")

loan_data['emp_length_int'].unique()

type(loan_data['emp_length_int'][0])

#changing column of dataframe from string to numeric
loan_data['emp_length_int'] = pd.to_numeric(loan_data['emp_length_int'])

type(loan_data['emp_length_int'][0])

loan_data['term'].unique()

loan_data['term_int'] = loan_data['term'].str.replace(' months', '')

loan_data['term_int'].unique()

#Strip Leading and Trailing Space of the column in pandas
loan_data['term_int'] = loan_data['term_int'].str.strip()
print (loan_data['term_int'])

#Strip Leading Space 
#df1['State'] = df1['State'].str.lstrip()
#Strip Trailing Space
#df1['State'] = df1['State'].str.rstrip()
#Strip all the spaces of column
#df1['State'] = df1['State'].str.replace(" ","")

loan_data['term_int'] = pd.to_numeric(loan_data['term_int'])

print(type(loan_data['earliest_cr_line'][0]))
loan_data['earliest_cr_line'] #this should be a date variable not a string variable

loan_data['earliest_cr_line_date'] = pd.to_datetime(loan_data['earliest_cr_line'], format = '%b-%y')

print(type(loan_data['earliest_cr_line_date'][0]))
loan_data['earliest_cr_line_date']

pd.to_datetime('2017-12-01')-loan_data['earliest_cr_line_date']
#number of days between the date of earliest credit line and december 2017

#we need the difference in months not days, so we divide my 30 and round it
loan_data['months_since_earliest_credit_line']=round(pd.to_numeric((pd.to_datetime('2017-12-01')-loan_data['earliest_cr_line_date'])/np.timedelta64(1,'M')))

loan_data['months_since_earliest_credit_line'].describe()
#negetive date differences are invalid values

#subsetting all rows and 3 columns where months_since_earliest_credit_line is negetive
loan_data.loc[:, ['earliest_cr_line','earliest_cr_line_date', 'months_since_earliest_credit_line']][loan_data['months_since_earliest_credit_line']<0]

loan_data['months_since_earliest_credit_line'][loan_data['months_since_earliest_credit_line']<0] = loan_data['months_since_earliest_credit_line'].max()

loan_data['months_since_earliest_credit_line'].min()

loan_data['months_since_earliest_credit_line'].describe()

loan_data['issue_d']

loan_data['issue_d_date'] = pd.to_datetime(loan_data['issue_d'], format = '%b-%y')

print(type(loan_data['issue_d_date'][0]))
loan_data['issue_d_date']

loan_data['mths_since_issue_d']=round(pd.to_numeric((pd.to_datetime('2017-12-01')-loan_data['issue_d_date'])/np.timedelta64(1,'M')))

loan_data['mths_since_issue_d'].describe()
#negetive date differences are invalid values

#### Prepocessing some discrete variables

loan_data['grade'].unique()

pd.get_dummies(loan_data['grade'])

pd.get_dummies(loan_data['grade'], prefix='grade', prefix_sep=':')

loan_data_dummies = [pd.get_dummies(loan_data['grade'], prefix='grade', prefix_sep=':'),
                     pd.get_dummies(loan_data['sub_grade'], prefix='sub_grade', prefix_sep=':'),
                     pd.get_dummies(loan_data['home_ownership'], prefix='home_ownership', prefix_sep=':'),
                     pd.get_dummies(loan_data['verification_status'], prefix='verification_status', prefix_sep=':'),
                     pd.get_dummies(loan_data['loan_status'], prefix='loan_status', prefix_sep=':'),
                     pd.get_dummies(loan_data['purpose'], prefix='purpose', prefix_sep=':'),
                     pd.get_dummies(loan_data['addr_state'], prefix='addr_state', prefix_sep=':'),
                     pd.get_dummies(loan_data['initial_list_status'], prefix='initial_list_status', prefix_sep=':')]

loan_data_dummies

loan_data_dummies = pd.concat(loan_data_dummies, axis = 1)

loan_data_dummies

type(loan_data_dummies)

#concatenate by column
loan_data=pd.concat([loan_data, loan_data_dummies], axis =1)

loan_data.columns.values

#### Check for missing values and clean

loan_data.isnull() #gives output as true and false

pd.options.display.max_rows = None
loan_data.isnull().sum()

#Deal with missing values - remove those obs or replace missing values
pd.options.display.max_rows = 100
loan_data['total_rev_hi_lim'].fillna(loan_data['funded_amnt'], inplace = True)

loan_data['total_rev_hi_lim'].isnull().sum()

loan_data['annual_inc'].isnull().sum()

loan_data['annual_inc'].notnull().sum()

loan_data['annual_inc'].mean()

loan_data['annual_inc'].fillna(loan_data['annual_inc'].mean(), inplace = True)

loan_data['months_since_earliest_credit_line'].fillna(0, inplace=True)
loan_data['acc_now_delinq'].fillna(0, inplace=True)
loan_data['total_acc'].fillna(0, inplace=True)
loan_data['pub_rec'].fillna(0, inplace=True)
loan_data['open_acc'].fillna(0, inplace=True)
loan_data['inq_last_6mths'].fillna(0, inplace=True)
loan_data['delinq_2yrs'].fillna(0, inplace=True)

#we can export this file as preprocessed data but we downloaded it from Udemy directly to use in LGD and EAD models
#loan_data.to_csv("C:/Users/vivek.goel/Downloads/Credit_Risk_Modelling_Udemy_course/loan_data_2007_2014_preprocessed.csv")

### PD Model

##### Working with dependent variable

loan_data['loan_status'].unique()

loan_data['loan_status'].value_counts()

loan_data['loan_status'].value_counts()/loan_data['loan_status'].count()

loan_data['good_bad']=np.where(loan_data['loan_status'].isin(['Charged Off','Default',
                                                              'Does not meet the credit policy. Status:Charged Off',
                                                              'Late (31-120 days)']), 0,1)

loan_data['good_bad']

##### Working with continuous variables

##### Splitting the data


from sklearn.model_selection import train_test_split

train_test_split(loan_data.drop('good_bad', axis = 1), loan_data['good_bad'])
#output of 4 arrays  - train data with input, test data with inputs, train dataset with target, test dataset with target

loan_data_inputs_train, loan_data_inputs_test , loan_data_targets_train,loan_data_targets_test = train_test_split(loan_data.drop('good_bad', axis = 1), loan_data['good_bad'])

print(loan_data_inputs_train.shape, loan_data_inputs_test.shape, loan_data_targets_train.shape,loan_data_targets_test.shape)

print(loan_data_inputs_train.shape[0]/loan_data.shape[0]) #75% of data as train dataset 

# splitting on 80-20
loan_data_inputs_train, loan_data_inputs_test , loan_data_targets_train,loan_data_targets_test = train_test_split(loan_data.drop('good_bad', axis = 1), loan_data['good_bad'], test_size=0.2)

#more obs in train but obs are shuffeled as well by default
print(loan_data_inputs_train.shape, loan_data_inputs_test.shape, loan_data_targets_train.shape,loan_data_targets_test.shape)

#splitting at 80-20 and set the random shuffling to 42
loan_data_inputs_train, loan_data_inputs_test , loan_data_targets_train,loan_data_targets_test = train_test_split(loan_data.drop('good_bad', axis = 1), loan_data['good_bad'], test_size=0.2, random_state=42)

###### For calculating weight of evidence: taking an example

#for training data we used both these commented commands
#df_inputs_prepr = loan_data_inputs_train  
#df_targets_prepr = loan_data_targets_train


#for preparing test data we used this
df_inputs_prepr = loan_data_inputs_test
df_targets_prepr = loan_data_targets_test

df_inputs_prepr['grade'].unique()

df1=pd.concat([df_inputs_prepr['grade'], df_targets_prepr], axis=1)

df1.head()

#find the proportion of good and bad borrowers by grade using group by
df1.groupby(df1.columns.values[0], as_index=False)[df1.columns.values[1]].count()

df1.groupby(df1.columns.values[0], as_index=False)[df1.columns.values[1]].mean()

df1=pd.concat([df1.groupby(df1.columns.values[0], as_index=False)[df1.columns.values[1]].count(),
               df1.groupby(df1.columns.values[0], as_index=False)[df1.columns.values[1]].mean()], axis = 1)

df1

#dropping the 3rd column
df1= df1.iloc[:, [0,1,3]]
df1

#renaming columns
df1.columns = [df1.columns.values[0], 'n_obs', 'prop_good']
df1

df1['prop_n_obs'] = df1['n_obs']/df1['n_obs'].sum()
df1

df1['n_good'] = df1['prop_good']*df1['n_obs']
df1

df1['n_bad'] = (1-df1['prop_good']) * df1['n_obs']
df1

df1['prop_n_good'] = df1['n_good']/df1['n_good'].sum()
df1['prop_n_bad'] = df1['n_bad']/df1['n_bad'].sum()
df1

df1['WoE'] = np.log(df1['prop_n_good']/df1['prop_n_bad'])
df1

df1 = df1.sort_values(['WoE'])
df1 = df1.reset_index(drop = True)
df1

df1['diff_prop_good'] = df1['prop_good'].diff().abs()
df1['diff_WoE'] = df1['WoE'].diff().abs()
df1

df1['IV'] = (df1['prop_n_good'] - df1['prop_n_bad']) * df1['WoE']
df1

df1['IV'] = df1['IV'].sum()
df1

#### Preprocessing Discrete variables : Automating Calculations

def woe_discrete(df, discrete_variable_name, good_bad_variable_df):
    df=pd.concat([df[discrete_variable_name], good_bad_variable_df], axis=1)
    df=pd.concat([df.groupby(df.columns.values[0], as_index=False)[df.columns.values[1]].count(),
                  df.groupby(df.columns.values[0], as_index=False)[df.columns.values[1]].mean()], axis = 1)
    df                   = df.iloc[:, [0,1,3]]
    df.columns           = [df.columns.values[0], 'n_obs', 'prop_good']
    df['prop_n_obs']     = df['n_obs']/df['n_obs'].sum()
    df['n_good']         = df['prop_good']*df['n_obs']
    df['n_bad']          = (1 - df['prop_good']) * df['n_obs']
    df['prop_n_good']    = df['n_good']/df['n_good'].sum()
    df['prop_n_bad']     = df['n_bad']/df['n_bad'].sum()
    df['WoE']            = np.log(df['prop_n_good']/df['prop_n_bad'])
    df                   = df.sort_values(['WoE'])
    df                   = df.reset_index(drop = True)
    df['diff_prop_good'] = df['prop_good'].diff().abs()
    df['diff_WoE']       = df['WoE'].diff().abs()
    df['IV']             = (df['prop_n_good'] - df['prop_n_bad']) * df['WoE']
    df['IV']             = df['IV'].sum()
    return df


df_temp = woe_discrete(df_inputs_prepr, 'grade', df_targets_prepr)
df_temp

#### Preprocessing Discrete variables : Visualiaing Results

import matplotlib.pyplot as plt
import seaborn as sns
sns.set()

def plot_by_woe(df_WoE, rotation_of_x_axis_labels=0):
    x = np.array(df_WoE.iloc[: , 0].apply(str))
    y = df_WoE['WoE']
    plt.figure(figsize=(18,6)) #width and height or X and Y
    plt.plot(x, y, marker = 'o', linestyle = '--' , color = 'k')
    plt.xlabel(df_WoE.columns[0])
    plt.ylabel('Weight of Evidence')
    plt.title(str('Weight of Evidence by ' + df_WoE.columns[0]))
    plt.xticks(rotation = rotation_of_x_axis_labels)

plot_by_woe(df_temp)

df_temp = woe_discrete(df_inputs_prepr, 'home_ownership', df_targets_prepr)
df_temp

plot_by_woe(df_temp)

#combining categories which have low number of obs and seems to be insignificant independently
#rent, other, none and any are combined to one
df_inputs_prepr['home_ownership:RENT_OTHER_NONE_ANY'] = sum([df_inputs_prepr['home_ownership:RENT'], df_inputs_prepr['home_ownership:OTHER'],
                                                            df_inputs_prepr['home_ownership:NONE'], df_inputs_prepr['home_ownership:ANY']])

df_inputs_prepr['home_ownership'].unique()

df_inputs_prepr['home_ownership:RENT_OTHER_NONE_ANY']

df_inputs_prepr['addr_state'].unique()

df_temp = woe_discrete(df_inputs_prepr, 'addr_state', df_targets_prepr)
df_temp

plot_by_woe(df_temp)

if ['addr_state:ND'] in df_inputs_prepr.columns.values:
    pass
else:
    df_inputs_prepr['addr_state:ND'] = 0

plot_by_woe(df_temp.iloc[2: -2, :])

#combining NE, IA, NV, FL, HI, AL as well as ND, which has no obs but can be risky, into one category
#combining WV, NH, WY, DC, ME, ID as one
#left with remaining 38 states, lets look at the graph
plot_by_woe(df_temp.iloc[6: -6, :])
    

#NM to CA have similar WoE, but lets look at the no. of obs for each NY and CA should be seperate
#so combine NM and VA
#next is NY
#combine states from OK to NC
#and then CA
#UT to NJ are similar..so combine then to one
#AR to MN as one, RI to IN as one, GA to OR as one, WI and MT in one group
#TX to CT show same WoE but TX no of obs is very high.. so TX is independent and combine IL and CT
#combine KS to MS in one group 
df_inputs_prepr['addr_state:ND_NE_IA_NV_FL_HI_AL'] = sum([df_inputs_prepr['addr_state:ND'], df_inputs_prepr['addr_state:NE'],
                                                         df_inputs_prepr['addr_state:IA'], df_inputs_prepr['addr_state:NV'],
                                                         df_inputs_prepr['addr_state:FL'], df_inputs_prepr['addr_state:HI'],
                                                         df_inputs_prepr['addr_state:AL']])

df_inputs_prepr['addr_state:NM_VA'] = sum([df_inputs_prepr['addr_state:NM'], df_inputs_prepr['addr_state:VA']])

df_inputs_prepr['addr_state:OK_TN_MO_LA_MD_NC'] = sum([df_inputs_prepr['addr_state:OK'], df_inputs_prepr['addr_state:TN'],
                                                         df_inputs_prepr['addr_state:MO'], df_inputs_prepr['addr_state:LA'],
                                                         df_inputs_prepr['addr_state:MD'], df_inputs_prepr['addr_state:NC']])

df_inputs_prepr['addr_state:UT_KY_AZ_NJ'] = sum([df_inputs_prepr['addr_state:UT'], df_inputs_prepr['addr_state:KY'],
                                                         df_inputs_prepr['addr_state:AZ'], df_inputs_prepr['addr_state:NJ']])

df_inputs_prepr['addr_state:AR_MI_PA_OH_MN'] = sum([df_inputs_prepr['addr_state:AR'], df_inputs_prepr['addr_state:OH'],
                                                         df_inputs_prepr['addr_state:MI'], df_inputs_prepr['addr_state:MN'], 
                                                    df_inputs_prepr['addr_state:PA']])

df_inputs_prepr['addr_state:RI_MA_DE_SD_IN'] = sum([df_inputs_prepr['addr_state:RI'], df_inputs_prepr['addr_state:SD'],
                                                         df_inputs_prepr['addr_state:MA'], df_inputs_prepr['addr_state:IN'], 
                                                    df_inputs_prepr['addr_state:DE']])

df_inputs_prepr['addr_state:GA_WA_OR'] = sum([df_inputs_prepr['addr_state:GA'], df_inputs_prepr['addr_state:WA'], 
                                          df_inputs_prepr['addr_state:OR']])

df_inputs_prepr['addr_state:WI_MT'] = sum([df_inputs_prepr['addr_state:WI'], df_inputs_prepr['addr_state:MT']])

df_inputs_prepr['addr_state:IL_CT'] = sum([df_inputs_prepr['addr_state:IL'], df_inputs_prepr['addr_state:CT']])

df_inputs_prepr['addr_state:KS_SC_CO_VT_AK_MS'] = sum([df_inputs_prepr['addr_state:KS'], df_inputs_prepr['addr_state:VT'],
                                                         df_inputs_prepr['addr_state:SC'], df_inputs_prepr['addr_state:AK'],
                                                         df_inputs_prepr['addr_state:CO'], df_inputs_prepr['addr_state:MS']])

df_inputs_prepr['addr_state:WV_NH_WY_DC_ME_ID'] = sum([df_inputs_prepr['addr_state:WV'], df_inputs_prepr['addr_state:DC'],
                                                         df_inputs_prepr['addr_state:NH'], df_inputs_prepr['addr_state:ME'],
                                                         df_inputs_prepr['addr_state:WY'], df_inputs_prepr['addr_state:ID']])

df_temp = woe_discrete(df_inputs_prepr, 'verification_status', df_targets_prepr)
df_temp

plot_by_woe(df_temp)

df_temp = woe_discrete(df_inputs_prepr, 'purpose', df_targets_prepr)
df_temp

plot_by_woe(df_temp, 45)

# We combine 'educational', 'small_business', 'wedding', 'renewable_energy', 'moving', 'house' in one category: 'educ__sm_b__wedd__ren_en__mov__house'.
# We combine 'other', 'medical', 'vacation' in one category: 'oth__med__vacation'.
# We combine 'major_purchase', 'car', 'home_improvement' in one category: 'major_purch__car__home_impr'.
# We leave 'debt_consolidtion' in a separate category.
    # We leave 'credit_card' in a separate category.
# 'educ__sm_b__wedd__ren_en__mov__house' will be the reference category.

df_inputs_prepr['purpose:educ__sm_b__wedd__ren_en__mov__house'] = sum([df_inputs_prepr['purpose:educational'], df_inputs_prepr['purpose:small_business'],
                                                                 df_inputs_prepr['purpose:wedding'], df_inputs_prepr['purpose:renewable_energy'],
                                                                 df_inputs_prepr['purpose:moving'], df_inputs_prepr['purpose:house']])
df_inputs_prepr['purpose:oth__med__vacation'] = sum([df_inputs_prepr['purpose:other'], df_inputs_prepr['purpose:medical'],
                                             df_inputs_prepr['purpose:vacation']])
df_inputs_prepr['purpose:major_purch__car__home_impr'] = sum([df_inputs_prepr['purpose:major_purchase'], df_inputs_prepr['purpose:car'],
                                                        df_inputs_prepr['purpose:home_improvement']])



df_temp = woe_discrete(df_inputs_prepr, 'initial_list_status', df_targets_prepr)
df_temp

plot_by_woe(df_temp)

#### Preprocessing Continuous variables : Automating Calculations & Visualizing Results

def woe_ordered_continuous(df, discrete_variable_name, good_bad_variable_df):
    df=pd.concat([df[discrete_variable_name], good_bad_variable_df], axis=1)
    df=pd.concat([df.groupby(df.columns.values[0], as_index=False)[df.columns.values[1]].count(),
                  df.groupby(df.columns.values[0], as_index=False)[df.columns.values[1]].mean()], axis = 1)
    df                   = df.iloc[:, [0,1,3]]
    df.columns           = [df.columns.values[0], 'n_obs', 'prop_good']
    df['prop_n_obs']     = df['n_obs']/df['n_obs'].sum()
    df['n_good']         = df['prop_good']*df['n_obs']
    df['n_bad']          = (1 - df['prop_good']) * df['n_obs']
    df['prop_n_good']    = df['n_good']/df['n_good'].sum()
    df['prop_n_bad']     = df['n_bad']/df['n_bad'].sum()
    df['WoE']            = np.log(df['prop_n_good']/df['prop_n_bad'])
    #df                   = df.sort_values(['WoE'])
    #df                   = df.reset_index(drop = True)
    df['diff_prop_good'] = df['prop_good'].diff().abs()
    df['diff_WoE']       = df['WoE'].diff().abs()
    df['IV']             = (df['prop_n_good'] - df['prop_n_bad']) * df['WoE']
    df['IV']             = df['IV'].sum()
    return df


#Creating Dummy variable
df_inputs_prepr['term_int'].unique()

df_temp = woe_ordered_continuous(df_inputs_prepr, 'term_int', df_targets_prepr)
df_temp

plot_by_woe(df_temp) #60 months loan are much riskier

df_inputs_prepr['term:36'] = np.where((df_inputs_prepr['term_int']==36), 1, 0)
df_inputs_prepr['term:60'] = np.where((df_inputs_prepr['term_int']==60), 1, 0)

df_inputs_prepr['emp_length_int'].unique()

df_temp = woe_ordered_continuous(df_inputs_prepr, 'emp_length_int', df_targets_prepr)
df_temp

plot_by_woe(df_temp)

df_inputs_prepr['emp_length_int:0'] = np.where(df_inputs_prepr['emp_length_int'].isin([0]), 1, 0)
df_inputs_prepr['emp_length_int:1'] = np.where(df_inputs_prepr['emp_length_int'].isin([1]), 1, 0)
df_inputs_prepr['emp_length_int:2-4'] = np.where(df_inputs_prepr['emp_length_int'].isin(range(2,5)), 1, 0)
df_inputs_prepr['emp_length_int:5-6'] = np.where(df_inputs_prepr['emp_length_int'].isin(range(5,7)), 1, 0)
df_inputs_prepr['emp_length_int:7-9'] = np.where(df_inputs_prepr['emp_length_int'].isin(range(7,10)), 1, 0)
df_inputs_prepr['emp_length_int:10'] = np.where(df_inputs_prepr['emp_length_int'].isin([10]), 1, 0)

df_inputs_prepr['mths_since_issue_d'].unique()

#fine classing
df_inputs_prepr['mths_since_issue_d_factor'] = pd.cut(df_inputs_prepr['mths_since_issue_d'], 50)
df_inputs_prepr['mths_since_issue_d_factor']

df_inputs_prepr['mths_since_issue_d_factor'].unique()

df_temp = woe_ordered_continuous(df_inputs_prepr, 'mths_since_issue_d_factor', df_targets_prepr)
df_temp

plot_by_woe(df_temp,90)

plot_by_woe(df_temp.iloc[3: , :], 90)

df_inputs_prepr['mths_since_issue_d:<38'] = np.where(df_inputs_prepr['mths_since_issue_d'].isin(range(38)), 1, 0)
df_inputs_prepr['mths_since_issue_d:38-39'] = np.where(df_inputs_prepr['mths_since_issue_d'].isin(range(38,40)), 1, 0)
df_inputs_prepr['mths_since_issue_d:40-41'] = np.where(df_inputs_prepr['mths_since_issue_d'].isin(range(40,42)), 1, 0)
df_inputs_prepr['mths_since_issue_d:42-48'] = np.where(df_inputs_prepr['mths_since_issue_d'].isin(range(42,49)), 1, 0)
df_inputs_prepr['mths_since_issue_d:49-52'] = np.where(df_inputs_prepr['mths_since_issue_d'].isin(range(49,53)), 1, 0)
df_inputs_prepr['mths_since_issue_d:53-64'] = np.where(df_inputs_prepr['mths_since_issue_d'].isin(range(53,65)), 1, 0)
df_inputs_prepr['mths_since_issue_d:65-84'] = np.where(df_inputs_prepr['mths_since_issue_d'].isin(range(65,85)), 1, 0)
df_inputs_prepr['mths_since_issue_d:>84'] = np.where(df_inputs_prepr['mths_since_issue_d'].isin(range(85, int(df_inputs_prepr['mths_since_issue_d'].max()))), 1, 0)

df_inputs_prepr['int_rate'].unique() 

df_inputs_prepr['int_rate_factor'] = pd.cut(df_inputs_prepr['int_rate'], 50)
df_inputs_prepr['int_rate_factor']

df_temp=woe_ordered_continuous(df_inputs_prepr, 'int_rate_factor',df_targets_prepr)
df_temp

plot_by_woe(df_temp, 90)

df_inputs_prepr['int_rate:<9.548'] = np.where((df_inputs_prepr['int_rate']<=9.548), 1, 0)
df_inputs_prepr['int_rate:9.548-12.025'] = np.where((df_inputs_prepr['int_rate']>9.548 ) & (df_inputs_prepr['int_rate']<=12.025), 1, 0)
df_inputs_prepr['int_rate:12.025-15.74'] = np.where((df_inputs_prepr['int_rate']>12.025 ) & (df_inputs_prepr['int_rate']<=15.74), 1, 0)
df_inputs_prepr['int_rate:15.74-20.281'] = np.where((df_inputs_prepr['int_rate']>15.74 ) & (df_inputs_prepr['int_rate']<=20.281), 1, 0)
df_inputs_prepr['int_rate:>20.281'] = np.where((df_inputs_prepr['int_rate']>20.281), 1, 0)

df_inputs_prepr['funded_amnt_factor'] = pd.cut(df_inputs_prepr['funded_amnt'], 50)
df_temp = woe_ordered_continuous(df_inputs_prepr, 'funded_amnt_factor', df_targets_prepr )
df_temp

plot_by_woe(df_temp, 90) #almost horizontal, not enough association with the dependent variable. so no need to include 

# mths_since_earliest_cr_line
df_inputs_prepr['months_since_earliest_credit_line_factor'] = pd.cut(df_inputs_prepr['months_since_earliest_credit_line'], 50)
# Here we do fine-classing: using the 'cut' method, we split the variable into 50 categories by its values.

df_temp = woe_ordered_continuous(df_inputs_prepr, 'months_since_earliest_credit_line_factor', df_targets_prepr)
df_temp

plot_by_woe(df_temp, 90)
# We plot the weight of evidence values.

plot_by_woe(df_temp.iloc[6: , : ], 90)
# We plot the weight of evidence values.

# We create the following categories:
# < 140, # 141 - 164, # 165 - 247, # 248 - 270, # 271 - 352, # > 352
df_inputs_prepr['months_since_earliest_credit_line:<140'] = np.where(df_inputs_prepr['months_since_earliest_credit_line'].isin(range(140)), 1, 0)
df_inputs_prepr['months_since_earliest_credit_line:141-164'] = np.where(df_inputs_prepr['months_since_earliest_credit_line'].isin(range(140, 165)), 1, 0)
df_inputs_prepr['months_since_earliest_credit_line:165-247'] = np.where(df_inputs_prepr['months_since_earliest_credit_line'].isin(range(165, 248)), 1, 0)
df_inputs_prepr['months_since_earliest_credit_line:248-270'] = np.where(df_inputs_prepr['months_since_earliest_credit_line'].isin(range(248, 271)), 1, 0)
df_inputs_prepr['months_since_earliest_credit_line:271-352'] = np.where(df_inputs_prepr['months_since_earliest_credit_line'].isin(range(271, 353)), 1, 0)
df_inputs_prepr['months_since_earliest_credit_line:>352'] = np.where(df_inputs_prepr['months_since_earliest_credit_line'].isin(range(353, int(df_inputs_prepr['months_since_earliest_credit_line'].max()))), 1, 0)

# delinq_2yrs
df_temp = woe_ordered_continuous(df_inputs_prepr, 'delinq_2yrs', df_targets_prepr)
# We calculate weight of evidence.
df_temp

plot_by_woe(df_temp)
# We plot the weight of evidence values.

# Categories: 0, 1-3, >=4
df_inputs_prepr['delinq_2yrs:0'] = np.where((df_inputs_prepr['delinq_2yrs'] == 0), 1, 0)
df_inputs_prepr['delinq_2yrs:1-3'] = np.where((df_inputs_prepr['delinq_2yrs'] >= 1) & (df_inputs_prepr['delinq_2yrs'] <= 3), 1, 0)
df_inputs_prepr['delinq_2yrs:>=4'] = np.where((df_inputs_prepr['delinq_2yrs'] >= 9), 1, 0)

# inq_last_6mths
df_temp = woe_ordered_continuous(df_inputs_prepr, 'inq_last_6mths', df_targets_prepr)
# We calculate weight of evidence.
df_temp

plot_by_woe(df_temp)
# We plot the weight of evidence values.

# Categories: 0, 1 - 2, 3 - 6, > 6
df_inputs_prepr['inq_last_6mths:0'] = np.where((df_inputs_prepr['inq_last_6mths'] == 0), 1, 0)
df_inputs_prepr['inq_last_6mths:1-2'] = np.where((df_inputs_prepr['inq_last_6mths'] >= 1) & (df_inputs_prepr['inq_last_6mths'] <= 2), 1, 0)
df_inputs_prepr['inq_last_6mths:3-6'] = np.where((df_inputs_prepr['inq_last_6mths'] >= 3) & (df_inputs_prepr['inq_last_6mths'] <= 6), 1, 0)
df_inputs_prepr['inq_last_6mths:>6'] = np.where((df_inputs_prepr['inq_last_6mths'] > 6), 1, 0)

# open_acc
df_temp = woe_ordered_continuous(df_inputs_prepr, 'open_acc', df_targets_prepr)
# We calculate weight of evidence.
df_temp

plot_by_woe(df_temp, 90)
# We plot the weight of evidence values.

plot_by_woe(df_temp.iloc[ : 40, :], 90)
# We plot the weight of evidence values.

# Categories: '0', '1-3', '4-12', '13-17', '18-22', '23-25', '26-30', '>30'
df_inputs_prepr['open_acc:0'] = np.where((df_inputs_prepr['open_acc'] == 0), 1, 0)
df_inputs_prepr['open_acc:1-3'] = np.where((df_inputs_prepr['open_acc'] >= 1) & (df_inputs_prepr['open_acc'] <= 3), 1, 0)
df_inputs_prepr['open_acc:4-12'] = np.where((df_inputs_prepr['open_acc'] >= 4) & (df_inputs_prepr['open_acc'] <= 12), 1, 0)
df_inputs_prepr['open_acc:13-17'] = np.where((df_inputs_prepr['open_acc'] >= 13) & (df_inputs_prepr['open_acc'] <= 17), 1, 0)
df_inputs_prepr['open_acc:18-22'] = np.where((df_inputs_prepr['open_acc'] >= 18) & (df_inputs_prepr['open_acc'] <= 22), 1, 0)
df_inputs_prepr['open_acc:23-25'] = np.where((df_inputs_prepr['open_acc'] >= 23) & (df_inputs_prepr['open_acc'] <= 25), 1, 0)
df_inputs_prepr['open_acc:26-30'] = np.where((df_inputs_prepr['open_acc'] >= 26) & (df_inputs_prepr['open_acc'] <= 30), 1, 0)
df_inputs_prepr['open_acc:>=31'] = np.where((df_inputs_prepr['open_acc'] >= 31), 1, 0)

# pub_rec
df_temp = woe_ordered_continuous(df_inputs_prepr, 'pub_rec', df_targets_prepr)
# We calculate weight of evidence.
df_temp

plot_by_woe(df_temp, 90)
# We plot the weight of evidence values.

# Categories '0-2', '3-4', '>=5'
df_inputs_prepr['pub_rec:0-2'] = np.where((df_inputs_prepr['pub_rec'] >= 0) & (df_inputs_prepr['pub_rec'] <= 2), 1, 0)
df_inputs_prepr['pub_rec:3-4'] = np.where((df_inputs_prepr['pub_rec'] >= 3) & (df_inputs_prepr['pub_rec'] <= 4), 1, 0)
df_inputs_prepr['pub_rec:>=5'] = np.where((df_inputs_prepr['pub_rec'] >= 5), 1, 0)

# total_acc
df_inputs_prepr['total_acc_factor'] = pd.cut(df_inputs_prepr['total_acc'], 50)
# Here we do fine-classing: using the 'cut' method, we split the variable into 50 categories by its values.
df_temp = woe_ordered_continuous(df_inputs_prepr, 'total_acc_factor', df_targets_prepr)
# We calculate weight of evidence.
df_temp

plot_by_woe(df_temp, 90)
# We plot the weight of evidence values.

# Categories: '<=27', '28-51', '>51'
df_inputs_prepr['total_acc:<=27'] = np.where((df_inputs_prepr['total_acc'] <= 27), 1, 0)
df_inputs_prepr['total_acc:28-51'] = np.where((df_inputs_prepr['total_acc'] >= 28) & (df_inputs_prepr['total_acc'] <= 51), 1, 0)
df_inputs_prepr['total_acc:>=52'] = np.where((df_inputs_prepr['total_acc'] >= 52), 1, 0)

# acc_now_delinq
df_temp = woe_ordered_continuous(df_inputs_prepr, 'acc_now_delinq', df_targets_prepr)
# We calculate weight of evidence.
df_temp

plot_by_woe(df_temp)
# We plot the weight of evidence values.

# Categories: '0', '>=1'
df_inputs_prepr['acc_now_delinq:0'] = np.where((df_inputs_prepr['acc_now_delinq'] == 0), 1, 0)
df_inputs_prepr['acc_now_delinq:>=1'] = np.where((df_inputs_prepr['acc_now_delinq'] >= 1), 1, 0)

# total_rev_hi_lim
df_inputs_prepr['total_rev_hi_lim_factor'] = pd.cut(df_inputs_prepr['total_rev_hi_lim'], 2000)
# Here we do fine-classing: using the 'cut' method, we split the variable into 2000 categories by its values.
df_temp = woe_ordered_continuous(df_inputs_prepr, 'total_rev_hi_lim_factor', df_targets_prepr)
# We calculate weight of evidence.
df_temp

plot_by_woe(df_temp.iloc[: 50, : ], 90)
# We plot the weight of evidence values.

# Categories
# '<=5K', '5K-10K', '10K-20K', '20K-30K', '30K-40K', '40K-55K', '55K-95K', '>95K'
df_inputs_prepr['total_rev_hi_lim:<=5K'] = np.where((df_inputs_prepr['total_rev_hi_lim'] <= 5000), 1, 0)
df_inputs_prepr['total_rev_hi_lim:5K-10K'] = np.where((df_inputs_prepr['total_rev_hi_lim'] > 5000) & (df_inputs_prepr['total_rev_hi_lim'] <= 10000), 1, 0)
df_inputs_prepr['total_rev_hi_lim:10K-20K'] = np.where((df_inputs_prepr['total_rev_hi_lim'] > 10000) & (df_inputs_prepr['total_rev_hi_lim'] <= 20000), 1, 0)
df_inputs_prepr['total_rev_hi_lim:20K-30K'] = np.where((df_inputs_prepr['total_rev_hi_lim'] > 20000) & (df_inputs_prepr['total_rev_hi_lim'] <= 30000), 1, 0)
df_inputs_prepr['total_rev_hi_lim:30K-40K'] = np.where((df_inputs_prepr['total_rev_hi_lim'] > 30000) & (df_inputs_prepr['total_rev_hi_lim'] <= 40000), 1, 0)
df_inputs_prepr['total_rev_hi_lim:40K-55K'] = np.where((df_inputs_prepr['total_rev_hi_lim'] > 40000) & (df_inputs_prepr['total_rev_hi_lim'] <= 55000), 1, 0)
df_inputs_prepr['total_rev_hi_lim:55K-95K'] = np.where((df_inputs_prepr['total_rev_hi_lim'] > 55000) & (df_inputs_prepr['total_rev_hi_lim'] <= 95000), 1, 0)
df_inputs_prepr['total_rev_hi_lim:>95K'] = np.where((df_inputs_prepr['total_rev_hi_lim'] > 95000), 1, 0)

# installment
df_inputs_prepr['installment_factor'] = pd.cut(df_inputs_prepr['installment'], 50)
# Here we do fine-classing: using the 'cut' method, we split the variable into 50 categories by its values.
df_temp = woe_ordered_continuous(df_inputs_prepr, 'installment_factor', df_targets_prepr)
# We calculate weight of evidence.
df_temp

plot_by_woe(df_temp, 90)
# We plot the weight of evidence values.

df_inputs_prepr['annual_inc_factor'] = pd.cut(df_inputs_prepr['annual_inc'], 50)
df_temp = woe_ordered_continuous(df_inputs_prepr, 'annual_inc_factor', df_targets_prepr)
df_temp
#94% of obs in 1st interval...need to make 100 intervals 

df_inputs_prepr['annual_inc_factor'] = pd.cut(df_inputs_prepr['annual_inc'], 100)
df_temp = woe_ordered_continuous(df_inputs_prepr, 'annual_inc_factor', df_targets_prepr)
df_temp

df_inputs_prepr_temp = df_inputs_prepr.loc[df_inputs_prepr['annual_inc']<=140000, :]

df_inputs_prepr_temp

df_inputs_prepr_temp['annual_inc_factor'] =pd.cut(df_inputs_prepr_temp['annual_inc'], 50)
df_temp = woe_ordered_continuous(df_inputs_prepr_temp, 'annual_inc_factor', df_targets_prepr[df_inputs_prepr_temp.index])
df_temp

plot_by_woe(df_temp, 90)

df_inputs_prepr['annual_inc:<20K']     = np.where((df_inputs_prepr['annual_inc']<= 20000), 1, 0)
df_inputs_prepr['annual_inc:20K-30K']  = np.where((df_inputs_prepr['annual_inc'] > 20000) & (df_inputs_prepr['annual_inc'] <= 30000), 1, 0)
df_inputs_prepr['annual_inc:30K-40K']  = np.where((df_inputs_prepr['annual_inc'] > 30000) & (df_inputs_prepr['annual_inc'] <= 40000), 1, 0)
df_inputs_prepr['annual_inc:40K-50K']  = np.where((df_inputs_prepr['annual_inc'] > 40000) & (df_inputs_prepr['annual_inc'] <= 50000), 1, 0)
df_inputs_prepr['annual_inc:50K-60K']  = np.where((df_inputs_prepr['annual_inc'] > 50000) & (df_inputs_prepr['annual_inc'] <= 60000), 1, 0)
df_inputs_prepr['annual_inc:60K-70K']  = np.where((df_inputs_prepr['annual_inc'] > 60000) & (df_inputs_prepr['annual_inc'] <= 70000), 1, 0)
df_inputs_prepr['annual_inc:70K-80K']  = np.where((df_inputs_prepr['annual_inc'] > 70000) & (df_inputs_prepr['annual_inc'] <= 80000), 1, 0)
df_inputs_prepr['annual_inc:80K-90K']  = np.where((df_inputs_prepr['annual_inc'] > 80000) & (df_inputs_prepr['annual_inc'] <= 90000), 1, 0)
df_inputs_prepr['annual_inc:90K-100K'] = np.where((df_inputs_prepr['annual_inc'] > 90000) & (df_inputs_prepr['annual_inc'] <= 100000), 1, 0)
df_inputs_prepr['annual_inc:100K-120K']= np.where((df_inputs_prepr['annual_inc'] > 100000) & (df_inputs_prepr['annual_inc'] <=120000), 1, 0)
df_inputs_prepr['annual_inc:120K-140K']= np.where((df_inputs_prepr['annual_inc'] > 120000) & (df_inputs_prepr['annual_inc'] <= 140000), 1, 0) 
df_inputs_prepr['annual_inc:>140K']    = np.where((df_inputs_prepr['annual_inc'] > 140000), 1, 0)

#mths_since_last_delinq
df_inputs_prepr['mths_since_last_delinq'].isnull().sum()
#very large number of missing values
#so lets make dummy variable  for misssing values, 1 when value is missing, 0 when value is not missing

df_inputs_prepr_temp = df_inputs_prepr[pd.notnull(df_inputs_prepr['mths_since_last_delinq'])]

df_inputs_prepr_temp['mths_since_last_delinq_factor'] = pd.cut(df_inputs_prepr_temp['mths_since_last_delinq'], 50)
df_temp = woe_ordered_continuous(df_inputs_prepr_temp, 'mths_since_last_delinq_factor', df_targets_prepr[df_inputs_prepr_temp.index])
df_temp

plot_by_woe(df_temp, 90)

df_inputs_prepr['mths_since_last_delinq:Missing'] = np.where((df_inputs_prepr['mths_since_last_delinq'].isnull()), 1, 0)
df_inputs_prepr['mths_since_last_delinq:0-3'] = np.where((df_inputs_prepr['mths_since_last_delinq'] >= 0) & (df_inputs_prepr['pub_rec'] <= 3), 1, 0)
df_inputs_prepr['mths_since_last_delinq:4-30'] = np.where((df_inputs_prepr['mths_since_last_delinq'] >= 4) & (df_inputs_prepr['pub_rec'] <= 30), 1, 0)
df_inputs_prepr['mths_since_last_delinq:31-56'] = np.where((df_inputs_prepr['mths_since_last_delinq'] >= 31) & (df_inputs_prepr['pub_rec'] <= 56), 1, 0)
df_inputs_prepr['mths_since_last_delinq:>=57'] = np.where((df_inputs_prepr['mths_since_last_delinq'] >= 57), 1, 0)

# dti
df_inputs_prepr['dti_factor'] = pd.cut(df_inputs_prepr['dti'], 100)
# Here we do fine-classing: using the 'cut' method, we split the variable into 100 categories by its values.
df_temp = woe_ordered_continuous(df_inputs_prepr, 'dti_factor', df_targets_prepr)
# We calculate weight of evidence.
df_temp

plot_by_woe(df_temp, 90)
# We plot the weight of evidence values.

# Similarly to income, initial examination shows that most values are lower than 200.
# Hence, we are going to have one category for more than 35, and we are going to apply our approach to determine
# the categories of everyone with 150k or less.
df_inputs_prepr_temp = df_inputs_prepr.loc[df_inputs_prepr['dti'] <= 35, : ]

df_inputs_prepr_temp['dti_factor'] = pd.cut(df_inputs_prepr_temp['dti'], 50)
# Here we do fine-classing: using the 'cut' method, we split the variable into 50 categories by its values.
df_temp = woe_ordered_continuous(df_inputs_prepr_temp, 'dti_factor', df_targets_prepr[df_inputs_prepr_temp.index])
# We calculate weight of evidence.
df_temp

plot_by_woe(df_temp, 90)
# We plot the weight of evidence values.

# Categories:
df_inputs_prepr['dti:<=1.4'] = np.where((df_inputs_prepr['dti'] <= 1.4), 1, 0)
df_inputs_prepr['dti:1.4-3.5'] = np.where((df_inputs_prepr['dti'] > 1.4) & (df_inputs_prepr['dti'] <= 3.5), 1, 0)
df_inputs_prepr['dti:3.5-7.7'] = np.where((df_inputs_prepr['dti'] > 3.5) & (df_inputs_prepr['dti'] <= 7.7), 1, 0)
df_inputs_prepr['dti:7.7-10.5'] = np.where((df_inputs_prepr['dti'] > 7.7) & (df_inputs_prepr['dti'] <= 10.5), 1, 0)
df_inputs_prepr['dti:10.5-16.1'] = np.where((df_inputs_prepr['dti'] > 10.5) & (df_inputs_prepr['dti'] <= 16.1), 1, 0)
df_inputs_prepr['dti:16.1-20.3'] = np.where((df_inputs_prepr['dti'] > 16.1) & (df_inputs_prepr['dti'] <= 20.3), 1, 0)
df_inputs_prepr['dti:20.3-21.7'] = np.where((df_inputs_prepr['dti'] > 20.3) & (df_inputs_prepr['dti'] <= 21.7), 1, 0)
df_inputs_prepr['dti:21.7-22.4'] = np.where((df_inputs_prepr['dti'] > 21.7) & (df_inputs_prepr['dti'] <= 22.4), 1, 0)
df_inputs_prepr['dti:22.4-35'] = np.where((df_inputs_prepr['dti'] > 22.4) & (df_inputs_prepr['dti'] <= 35), 1, 0)
df_inputs_prepr['dti:>35'] = np.where((df_inputs_prepr['dti'] > 35), 1, 0)

# mths_since_last_record
# We have to create one category for missing values and do fine and coarse classing for the rest.
df_inputs_prepr_temp = df_inputs_prepr[pd.notnull(df_inputs_prepr['mths_since_last_record'])]
#sum(loan_data_temp['mths_since_last_record'].isnull())
df_inputs_prepr_temp['mths_since_last_record_factor'] = pd.cut(df_inputs_prepr_temp['mths_since_last_record'], 50)
# Here we do fine-classing: using the 'cut' method, we split the variable into 50 categories by its values.
df_temp = woe_ordered_continuous(df_inputs_prepr_temp, 'mths_since_last_record_factor', df_targets_prepr[df_inputs_prepr_temp.index])
# We calculate weight of evidence.
df_temp

plot_by_woe(df_temp, 90)
# We plot the weight of evidence values.

# Categories: 'Missing', '0-2', '3-20', '21-31', '32-80', '81-86', '>86'
df_inputs_prepr['mths_since_last_record:Missing'] = np.where((df_inputs_prepr['mths_since_last_record'].isnull()), 1, 0)
df_inputs_prepr['mths_since_last_record:0-2'] = np.where((df_inputs_prepr['mths_since_last_record'] >= 0) & (df_inputs_prepr['mths_since_last_record'] <= 2), 1, 0)
df_inputs_prepr['mths_since_last_record:3-20'] = np.where((df_inputs_prepr['mths_since_last_record'] >= 3) & (df_inputs_prepr['mths_since_last_record'] <= 20), 1, 0)
df_inputs_prepr['mths_since_last_record:21-31'] = np.where((df_inputs_prepr['mths_since_last_record'] >= 21) & (df_inputs_prepr['mths_since_last_record'] <= 31), 1, 0)
df_inputs_prepr['mths_since_last_record:32-80'] = np.where((df_inputs_prepr['mths_since_last_record'] >= 32) & (df_inputs_prepr['mths_since_last_record'] <= 80), 1, 0)
df_inputs_prepr['mths_since_last_record:81-86'] = np.where((df_inputs_prepr['mths_since_last_record'] >= 81) & (df_inputs_prepr['mths_since_last_record'] <= 86), 1, 0)
df_inputs_prepr['mths_since_last_record:>86'] = np.where((df_inputs_prepr['mths_since_last_record'] > 86), 1, 0)

#### Processing the test dataset

#loan_data_inputs_train=df_inputs_prepr #uncomment to save the processed training dataset in original df and comment the below command
loan_data_inputs_test = df_inputs_prepr #we ran the preperocessing code again for test dataset, then ran this command commenting the command above

loan_data_inputs_test

loan_data_inputs_train.to_csv("C:/Users/vivek.goel/Downloads/Credit_Risk_Modelling_Udemy_course/loan_data_inputs_train.csv")

loan_data_inputs_test.to_csv("C:/Users/vivek.goel/Downloads/Credit_Risk_Modelling_Udemy_course/loan_data_inputs_test.csv")

loan_data_targets_test.to_csv("C:/Users/vivek.goel/Downloads/Credit_Risk_Modelling_Udemy_course/loan_data_targets_test.csv")
loan_data_targets_train.to_csv("C:/Users/vivek.goel/Downloads/Credit_Risk_Modelling_Udemy_course/loan_data_targets_train.csv")
