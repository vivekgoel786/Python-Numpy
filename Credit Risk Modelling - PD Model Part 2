# PD Model 

import numpy as np
import pandas as pd

loan_data_inputs_train = pd.read_csv('C:/Users/vivek.goel/Downloads/Credit_Risk_Modelling_Udemy_course/loan_data_inputs_train.csv', index_col=0)
loan_data_inputs_test = pd.read_csv('C:/Users/vivek.goel/Downloads/Credit_Risk_Modelling_Udemy_course/loan_data_inputs_test.csv', index_col=0)

loan_data_targets_train = pd.read_csv('C:/Users/vivek.goel/Downloads/Credit_Risk_Modelling_Udemy_course/loan_data_targets_train.csv', index_col=0, header=None)
loan_data_targets_test = pd.read_csv('C:/Users/vivek.goel/Downloads/Credit_Risk_Modelling_Udemy_course/loan_data_targets_test.csv', index_col=0, header=None)

## EDA

loan_data_inputs_train.head()

loan_data_targets_train.head()

print(loan_data_targets_train.shape, loan_data_inputs_train.shape)

print(loan_data_targets_test.shape, loan_data_inputs_test.shape)

### Selecting the features

inputs_train_with_ref_cat = loan_data_inputs_train.loc[:, ['grade:A',
'grade:B',
'grade:C',
'grade:D',
'grade:E',
'grade:F',
'grade:G',
'home_ownership:RENT_OTHER_NONE_ANY',
'home_ownership:OWN',
'home_ownership:MORTGAGE',
'addr_state:ND_NE_IA_NV_FL_HI_AL',
'addr_state:NM_VA',
'addr_state:NY',
'addr_state:OK_TN_MO_LA_MD_NC',
'addr_state:CA',
'addr_state:UT_KY_AZ_NJ',
'addr_state:AR_MI_PA_OH_MN',
'addr_state:RI_MA_DE_SD_IN',
'addr_state:GA_WA_OR',
'addr_state:WI_MT',
'addr_state:TX',
'addr_state:IL_CT',
'addr_state:KS_SC_CO_VT_AK_MS',
'addr_state:WV_NH_WY_DC_ME_ID',
'verification_status:Source Verified',
'verification_status:Not Verified',
'verification_status:Verified',
'initial_list_status:f',
'initial_list_status:w',
'purpose:educ__sm_b__wedd__ren_en__mov__house',
'purpose:oth__med__vacation',
'purpose:major_purch__car__home_impr',
'purpose:debt_consolidation',
'purpose:credit_card',
'term:36',
'term:60',
'emp_length_int:0',
'emp_length_int:1',
'emp_length_int:2-4',
'emp_length_int:5-6',
'emp_length_int:7-9',
'emp_length_int:10',
'mths_since_issue_d:<38',
'mths_since_issue_d:38-39',
'mths_since_issue_d:40-41',
'mths_since_issue_d:42-48',
'mths_since_issue_d:49-52',
'mths_since_issue_d:53-64',
'mths_since_issue_d:65-84',
'mths_since_issue_d:>84',
'int_rate:<9.548',
'int_rate:9.548-12.025',
'int_rate:12.025-15.74',
'int_rate:15.74-20.281',
'int_rate:>20.281',
'months_since_earliest_credit_line:<140',
'months_since_earliest_credit_line:141-164',
'months_since_earliest_credit_line:165-247',
'months_since_earliest_credit_line:248-270',
'months_since_earliest_credit_line:271-352',
'months_since_earliest_credit_line:>352',
'delinq_2yrs:0',
'delinq_2yrs:1-3',
'delinq_2yrs:>=4',
'inq_last_6mths:0',
'inq_last_6mths:1-2',
'inq_last_6mths:3-6',
'inq_last_6mths:>6',
'open_acc:0',
'open_acc:1-3',
'open_acc:4-12',
'open_acc:13-17',
'open_acc:18-22',
'open_acc:23-25',
'open_acc:26-30',
'open_acc:>=31',
'pub_rec:0-2',
'pub_rec:3-4',
'pub_rec:>=5',
'total_acc:<=27',
'total_acc:28-51',
'total_acc:>=52',
'acc_now_delinq:0',
'acc_now_delinq:>=1',
'total_rev_hi_lim:<=5K',
'total_rev_hi_lim:5K-10K',
'total_rev_hi_lim:10K-20K',
'total_rev_hi_lim:20K-30K',
'total_rev_hi_lim:30K-40K',
'total_rev_hi_lim:40K-55K',
'total_rev_hi_lim:55K-95K',
'total_rev_hi_lim:>95K',
'annual_inc:<20K',
'annual_inc:20K-30K',
'annual_inc:30K-40K',
'annual_inc:40K-50K',
'annual_inc:50K-60K',
'annual_inc:60K-70K',
'annual_inc:70K-80K',
'annual_inc:80K-90K',
'annual_inc:90K-100K',
'annual_inc:100K-120K',
'annual_inc:120K-140K',
'annual_inc:>140K',
'mths_since_last_delinq:Missing',
'mths_since_last_delinq:0-3',
'mths_since_last_delinq:4-30',
'mths_since_last_delinq:31-56',
'mths_since_last_delinq:>=57',
'dti:<=1.4',
'dti:1.4-3.5',
'dti:3.5-7.7',
'dti:7.7-10.5',
'dti:10.5-16.1',
'dti:16.1-20.3',
'dti:20.3-21.7',
'dti:21.7-22.4',
'dti:22.4-35',
'dti:>35',
'mths_since_last_record:Missing',
'mths_since_last_record:0-2',
'mths_since_last_record:3-20',
'mths_since_last_record:21-31',
'mths_since_last_record:32-80',
'mths_since_last_record:81-86',
'mths_since_last_record:>86']]

ref_categories = ['grade:G',
'home_ownership:RENT_OTHER_NONE_ANY',
'addr_state:ND_NE_IA_NV_FL_HI_AL',
'verification_status:Verified',
'initial_list_status:f',
'purpose:educ__sm_b__wedd__ren_en__mov__house',
'term:60',
'emp_length_int:0',
'mths_since_issue_d:>84',
'int_rate:>20.281',
'months_since_earliest_credit_line:<140',
'delinq_2yrs:>=4',
'inq_last_6mths:>6',
'open_acc:0',
'pub_rec:0-2',
'total_acc:<=27',
'acc_now_delinq:0',
'total_rev_hi_lim:<=5K',
'annual_inc:<20K',
'dti:>35',
'mths_since_last_delinq:0-3',
'mths_since_last_record:0-2']

#removing base category dummy variables from the df
inputs_train = inputs_train_with_ref_cat.drop(ref_categories, axis = 1)

inputs_train.head()

### Logistic Regression

from sklearn.linear_model import LogisticRegression
from sklearn import metrics

reg = LogisticRegression()

pd.options.display.max_rows = None

reg.fit(inputs_train, loan_data_targets_train)

reg.intercept_

reg.coef_

feature_name = inputs_train.columns.values

summary_table = pd.DataFrame(columns=['Feature name'], data=feature_name)
summary_table['Coefficients'] = np.transpose(reg.coef_)
summary_table

summary_table.index = summary_table.index + 1
summary_table.loc[0] = ['Intercept', reg.intercept_[0]]
summary_table = summary_table.sort_index()
summary_table

### Logistic Regression model with P-values

from sklearn import linear_model
import scipy.stats as stat


class LogisticRegression_with_p_values:
    
    def __init__(self,*args,**kwargs):
        self.model = linear_model.LogisticRegression(*args,**kwargs)
    
    def fit(self,X,y):
        self.model.fit(X,y)
        denom = (2.0 * (1.0 + np.cosh(self.model.decision_function(X))))
        denom = np.tile(denom, (X.shape[1],1)).T
        F_ij = np.dot((X/denom).T, X)
        Cramer_Rao = np.linalg.inv(F_ij)
        sigma_estimates = np.sqrt(np.diagonal(Cramer_Rao))
        z_scores = self.model.coef_[0]/sigma_estimates
        p_values = [stat.norm.sf(abs(x)) * 2 for x in z_scores]
        self.coef_ = self.model.coef_
        self.intercept_ = self.model.intercept_
        self.p_values = p_values

reg = LogisticRegression_with_p_values()

reg.fit(inputs_train, loan_data_targets_train)

summary_table = pd.DataFrame(columns=['Feature name'], data=feature_name)
summary_table['Coefficients'] = np.transpose(reg.coef_)
summary_table.index = summary_table.index + 1
summary_table.loc[0] = ['Intercept', reg.intercept_[0]]
summary_table = summary_table.sort_index()
summary_table

p_values = reg.p_values

p_values = np.append(np.nan, np.array(p_values))

summary_table['p_values'] = p_values

summary_table

 inputs_train_with_ref_cat = loan_data_inputs_train.loc[:, ['grade:A',
'grade:B',
'grade:C',
'grade:D',
'grade:E',
'grade:F',
'grade:G',
'home_ownership:RENT_OTHER_NONE_ANY',
'home_ownership:OWN',
'home_ownership:MORTGAGE',
'addr_state:ND_NE_IA_NV_FL_HI_AL',
'addr_state:NM_VA',
'addr_state:NY',
'addr_state:OK_TN_MO_LA_MD_NC',
'addr_state:CA',
'addr_state:UT_KY_AZ_NJ',
'addr_state:AR_MI_PA_OH_MN',
'addr_state:RI_MA_DE_SD_IN',
'addr_state:GA_WA_OR',
'addr_state:WI_MT',
'addr_state:TX',
'addr_state:IL_CT',
'addr_state:KS_SC_CO_VT_AK_MS',
'addr_state:WV_NH_WY_DC_ME_ID',
'verification_status:Source Verified',
'verification_status:Not Verified',
'verification_status:Verified',
'initial_list_status:f',
'initial_list_status:w',
'purpose:educ__sm_b__wedd__ren_en__mov__house',
'purpose:oth__med__vacation',
'purpose:major_purch__car__home_impr',
'purpose:debt_consolidation',
'purpose:credit_card',
'term:36',
'term:60',
'emp_length_int:0',
'emp_length_int:1',
'emp_length_int:2-4',
'emp_length_int:5-6',
'emp_length_int:7-9',
'emp_length_int:10',
'mths_since_issue_d:<38',
'mths_since_issue_d:38-39',
'mths_since_issue_d:40-41',
'mths_since_issue_d:42-48',
'mths_since_issue_d:49-52',
'mths_since_issue_d:53-64',
'mths_since_issue_d:65-84',
'mths_since_issue_d:>84',
'int_rate:<9.548',
'int_rate:9.548-12.025',
'int_rate:12.025-15.74',
'int_rate:15.74-20.281',
'int_rate:>20.281',
'months_since_earliest_credit_line:<140',
'months_since_earliest_credit_line:141-164',
'months_since_earliest_credit_line:165-247',
'months_since_earliest_credit_line:248-270',
'months_since_earliest_credit_line:271-352',
'months_since_earliest_credit_line:>352',
'inq_last_6mths:0',
'inq_last_6mths:1-2',
'inq_last_6mths:3-6',
'inq_last_6mths:>6',
'acc_now_delinq:0',
'acc_now_delinq:>=1',
'annual_inc:<20K',
'annual_inc:20K-30K',
'annual_inc:30K-40K',
'annual_inc:40K-50K',
'annual_inc:50K-60K',
'annual_inc:60K-70K',
'annual_inc:70K-80K',
'annual_inc:80K-90K',
'annual_inc:90K-100K',
'annual_inc:100K-120K',
'annual_inc:120K-140K',
'annual_inc:>140K',
'mths_since_last_delinq:Missing',
'mths_since_last_delinq:0-3',
'mths_since_last_delinq:4-30',
'mths_since_last_delinq:31-56',
'mths_since_last_delinq:>=57',
'dti:<=1.4',
'dti:1.4-3.5',
'dti:3.5-7.7',
'dti:7.7-10.5',
'dti:10.5-16.1',
'dti:16.1-20.3',
'dti:20.3-21.7',
'dti:21.7-22.4',
'dti:22.4-35',
'dti:>35',
'mths_since_last_record:Missing',
'mths_since_last_record:0-2',
'mths_since_last_record:3-20',
'mths_since_last_record:21-31',
'mths_since_last_record:32-80',
'mths_since_last_record:81-86',
'mths_since_last_record:>86']]

ref_categories = ['grade:G',
'home_ownership:RENT_OTHER_NONE_ANY',
'addr_state:ND_NE_IA_NV_FL_HI_AL',
'verification_status:Verified',
'initial_list_status:f',
'purpose:educ__sm_b__wedd__ren_en__mov__house',
'term:60',
'emp_length_int:0',
'mths_since_issue_d:>84',
'int_rate:>20.281',
'months_since_earliest_credit_line:<140',
'inq_last_6mths:>6',
'acc_now_delinq:0',
'annual_inc:<20K',
'dti:>35',
'mths_since_last_delinq:0-3',
'mths_since_last_record:0-2']

inputs_train = inputs_train_with_ref_cat.drop(ref_categories, axis = 1)
inputs_train.head()

reg2 = LogisticRegression_with_p_values()
reg2.fit(inputs_train, loan_data_targets_train)

feature_name = inputs_train.columns.values
feature_name

summary_table = pd.DataFrame(columns=['Feature name'], data=feature_name)
summary_table['Coefficients'] = np.transpose(reg2.coef_)
summary_table.index = summary_table.index + 1
summary_table.loc[0] = ['Intercept', reg2.intercept_[0]]
summary_table = summary_table.sort_index()
summary_table

p_values = reg2.p_values
p_values = np.append(np.nan, np.array(p_values))
summary_table['p_values'] = p_values
summary_table

## PD Model Validation 

#### Out of sample validation (test data validation)

inputs_test_with_ref_cat = loan_data_inputs_test.loc[: , ['grade:A',
'grade:B',
'grade:C',
'grade:D',
'grade:E',
'grade:F',
'grade:G',
'home_ownership:RENT_OTHER_NONE_ANY',
'home_ownership:OWN',
'home_ownership:MORTGAGE',
'addr_state:ND_NE_IA_NV_FL_HI_AL',
'addr_state:NM_VA',
'addr_state:NY',
'addr_state:OK_TN_MO_LA_MD_NC',
'addr_state:CA',
'addr_state:UT_KY_AZ_NJ',
'addr_state:AR_MI_PA_OH_MN',
'addr_state:RI_MA_DE_SD_IN',
'addr_state:GA_WA_OR',
'addr_state:WI_MT',
'addr_state:TX',
'addr_state:IL_CT',
'addr_state:KS_SC_CO_VT_AK_MS',
'addr_state:WV_NH_WY_DC_ME_ID',
'verification_status:Source Verified',
'verification_status:Not Verified',
'verification_status:Verified',
'initial_list_status:f',
'initial_list_status:w',
'purpose:educ__sm_b__wedd__ren_en__mov__house',
'purpose:oth__med__vacation',
'purpose:major_purch__car__home_impr',
'purpose:debt_consolidation',
'purpose:credit_card',
'term:36',
'term:60',
'emp_length_int:0',
'emp_length_int:1',
'emp_length_int:2-4',
'emp_length_int:5-6',
'emp_length_int:7-9',
'emp_length_int:10',
'mths_since_issue_d:<38',
'mths_since_issue_d:38-39',
'mths_since_issue_d:40-41',
'mths_since_issue_d:42-48',
'mths_since_issue_d:49-52',
'mths_since_issue_d:53-64',
'mths_since_issue_d:65-84',
'mths_since_issue_d:>84',
'int_rate:<9.548',
'int_rate:9.548-12.025',
'int_rate:12.025-15.74',
'int_rate:15.74-20.281',
'int_rate:>20.281',
'months_since_earliest_credit_line:<140',
'months_since_earliest_credit_line:141-164',
'months_since_earliest_credit_line:165-247',
'months_since_earliest_credit_line:248-270',
'months_since_earliest_credit_line:271-352',
'months_since_earliest_credit_line:>352',
'inq_last_6mths:0',
'inq_last_6mths:1-2',
'inq_last_6mths:3-6',
'inq_last_6mths:>6',
'acc_now_delinq:0',
'acc_now_delinq:>=1',
'annual_inc:<20K',
'annual_inc:20K-30K',
'annual_inc:30K-40K',
'annual_inc:40K-50K',
'annual_inc:50K-60K',
'annual_inc:60K-70K',
'annual_inc:70K-80K',
'annual_inc:80K-90K',
'annual_inc:90K-100K',
'annual_inc:100K-120K',
'annual_inc:120K-140K',
'annual_inc:>140K',
'mths_since_last_delinq:Missing',
'mths_since_last_delinq:0-3',
'mths_since_last_delinq:4-30',
'mths_since_last_delinq:31-56',
'mths_since_last_delinq:>=57',
'dti:<=1.4',
'dti:1.4-3.5',
'dti:3.5-7.7',
'dti:7.7-10.5',
'dti:10.5-16.1',
'dti:16.1-20.3',
'dti:20.3-21.7',
'dti:21.7-22.4',
'dti:22.4-35',
'dti:>35',
'mths_since_last_record:Missing',
'mths_since_last_record:0-2',
'mths_since_last_record:3-20',
'mths_since_last_record:21-31',
'mths_since_last_record:32-80',
'mths_since_last_record:81-86',
'mths_since_last_record:>86']]

ref_categories = ['grade:G',
'home_ownership:RENT_OTHER_NONE_ANY',
'addr_state:ND_NE_IA_NV_FL_HI_AL',
'verification_status:Verified',
'initial_list_status:f',
'purpose:educ__sm_b__wedd__ren_en__mov__house',
'term:60',
'emp_length_int:0',
'mths_since_issue_d:>84',
'int_rate:>20.281',
'months_since_earliest_credit_line:<140',
'inq_last_6mths:>6',
'acc_now_delinq:0',
'annual_inc:<20K',
'dti:>35',
'mths_since_last_delinq:0-3',
'mths_since_last_record:0-2']

inputs_test = inputs_test_with_ref_cat.drop(ref_categories, axis = 1)
inputs_test.head()

y_hat_test = reg2.model.predict(inputs_test)

y_hat_test

y_hat_test_proba = reg2.model.predict_proba(inputs_test)
y_hat_test_proba

y_hat_test_proba[: ][: , 1] 

y_hat_test_proba = y_hat_test_proba[: ][: , 1] 

y_hat_test_proba

loan_data_targets_test_temp = loan_data_targets_test

loan_data_targets_test_temp.reset_index(drop = True, inplace = True)

df_actual_predicted_probs = pd.concat([loan_data_targets_test_temp, pd.DataFrame(y_hat_test_proba)], axis = 1)

df_actual_predicted_probs.shape

df_actual_predicted_probs.columns = ['loan_data_targets_test', 'y_hat_test_proba']

df_actual_predicted_probs.index = loan_data_inputs_test.index

df_actual_predicted_probs.head()

### Accuracy and Area Under Curve

tr = 0.5
df_actual_predicted_probs['y_hat_test'] = np.where(df_actual_predicted_probs['y_hat_test_proba'] > tr, 1,0)

pd.crosstab(df_actual_predicted_probs['loan_data_targets_test'], df_actual_predicted_probs['y_hat_test'], rownames = ['Actuals'], colnames= ['Predicted'])

pd.crosstab(df_actual_predicted_probs['loan_data_targets_test'], df_actual_predicted_probs['y_hat_test'],
            rownames = ['Actuals'], colnames= ['Predicted']) / df_actual_predicted_probs.shape[0]

(pd.crosstab(df_actual_predicted_probs['loan_data_targets_test'], df_actual_predicted_probs['y_hat_test'],
            rownames = ['Actuals'], colnames= ['Predicted']) / df_actual_predicted_probs.shape[0]).iloc[0,0] + (pd.crosstab(df_actual_predicted_probs['loan_data_targets_test'], df_actual_predicted_probs['y_hat_test'],
            rownames = ['Actuals'], colnames= ['Predicted']) / df_actual_predicted_probs.shape[0]).iloc[1,1]

#try with tr = 0.9
#accuracy falls, but predicting number of people that can default increases which is good
#we need ROC curve to set appropriate tr
from sklearn.metrics import roc_curve, roc_auc_score

roc_curve(df_actual_predicted_probs['loan_data_targets_test'], df_actual_predicted_probs['y_hat_test_proba'])
#returns 3 arrays - False positive rate, True positive rate, threshold

fpr, tpr, threshold =  roc_curve(df_actual_predicted_probs['loan_data_targets_test'], df_actual_predicted_probs['y_hat_test_proba'])

import matplotlib.pyplot as plt
import seaborn as sns
sns.set()

plt.plot(fpr, tpr)
plt.plot(fpr, fpr, linestyle = '--', color = 'k')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ROC curve')

AUROC = roc_auc_score(df_actual_predicted_probs['loan_data_targets_test'], df_actual_predicted_probs['y_hat_test_proba'])
AUROC

####  Gini Coefficient and KS Test

df_actual_predicted_probs = df_actual_predicted_probs.sort_values('y_hat_test_proba')

df_actual_predicted_probs.head()

df_actual_predicted_probs.tail()

df_actual_predicted_probs = df_actual_predicted_probs.reset_index()

df_actual_predicted_probs.head()

df_actual_predicted_probs['Cumulative N Population'] = df_actual_predicted_probs.index + 1
df_actual_predicted_probs['Cumulative N Good'] = df_actual_predicted_probs['loan_data_targets_test'].cumsum()
df_actual_predicted_probs['Cumulative N Bad'] = df_actual_predicted_probs['Cumulative N Population'] - df_actual_predicted_probs['loan_data_targets_test'].cumsum()

df_actual_predicted_probs.head()

df_actual_predicted_probs['Cumulative Perc Population'] = df_actual_predicted_probs['Cumulative N Population'] / (df_actual_predicted_probs.shape[0])
df_actual_predicted_probs['Cumulative Perc Good'] = df_actual_predicted_probs['Cumulative N Good'] / df_actual_predicted_probs['loan_data_targets_test'].sum()
df_actual_predicted_probs['Cumulative Perc Bad'] = df_actual_predicted_probs['Cumulative N Bad'] / (df_actual_predicted_probs.shape[0] - df_actual_predicted_probs['loan_data_targets_test'].sum())

df_actual_predicted_probs.head()

df_actual_predicted_probs.tail()

plt.plot(df_actual_predicted_probs['Cumulative Perc Population'], df_actual_predicted_probs['Cumulative Perc Bad'])
plt.plot(df_actual_predicted_probs['Cumulative Perc Population'], df_actual_predicted_probs['Cumulative Perc Population'],
        linestyle = '--', color = 'k')
plt.xlabel('Cumulative % Population')
plt.ylabel('Cumulative % Bad')
plt.title('Gini')

#Gini = (AUROC*2) - 1
#AUROC = (Gini + 1)/2
Gini = (AUROC*2) - 1
Gini #40.44%

plt.plot(df_actual_predicted_probs['y_hat_test_proba'], df_actual_predicted_probs['Cumulative Perc Bad'], color = 'r')
plt.plot(df_actual_predicted_probs['y_hat_test_proba'], df_actual_predicted_probs['Cumulative Perc Good'], color = 'b')
plt.xlabel('Estimated Probability of being Good')
plt.ylabel('Cumulative %')
plt.title('Kolmogorov-Smirnov Test')

KS = max(df_actual_predicted_probs['Cumulative Perc Bad'] - df_actual_predicted_probs['Cumulative Perc Good'])
KS #30% not too high, the two cumulative distribution functions are sufficiently far away from each other and the model has satisfactory predictive power

#### Calculating PD of individual accounts

pd.options.display.max_columns = None

inputs_test_with_ref_cat.head()

summary_table

### Creating Scorecard

summary_table

ref_categories

df_ref_categories = pd.DataFrame(ref_categories, columns = ['Feature name'])
df_ref_categories['Coefficients'] = 0
df_ref_categories['p_values'] = np.nan
df_ref_categories

df_scorecard = pd.concat([summary_table, df_ref_categories])
df_scorecard = df_scorecard.reset_index()
df_scorecard

df_scorecard['Original feature name'] = df_scorecard['Feature name'].str.split(':').str[0]
df_scorecard

#minimum score = 300, max score = 850
min_score = 300
max_score = 850

df_scorecard.groupby('Original feature name')['Coefficients'].min()

min_sum_coef = df_scorecard.groupby('Original feature name')['Coefficients'].min().sum()
min_sum_coef

max_sum_coef = df_scorecard.groupby('Original feature name')['Coefficients'].max().sum()
max_sum_coef

df_scorecard['Score - Calculation'] = df_scorecard['Coefficients'] * (max_score - min_score) / (max_sum_coef - min_sum_coef)
df_scorecard

#intercept score = [((intercept_coef - min_score)/(max_sum_coef-min_sum_coef))*(max_score-min_score)] + min_score
df_scorecard['Score - Calculation'][0] = (((df_scorecard['Coefficients'][0] - min_sum_coef)/(max_sum_coef-min_sum_coef))*(max_score-min_score)) + min_score
df_scorecard

df_scorecard['Score - Preliminary'] = df_scorecard['Score - Calculation'].round()
df_scorecard

min_sum_score_prel = df_scorecard.groupby('Original feature name')['Score - Preliminary'].min().sum()
min_sum_score_prel

max_sum_score_prel = df_scorecard.groupby('Original feature name')['Score - Preliminary'].max().sum()
max_sum_score_prel

df_scorecard['Difference'] = df_scorecard['Score - Preliminary'] - df_scorecard['Score - Calculation']
df_scorecard

df_scorecard['Difference'].min()

df_scorecard['Score - Final'] = df_scorecard['Score - Preliminary']
df_scorecard['Score - Final'][0] = 317
#df_scorecard['Score - Final'][39] = 31
#df_scorecard['Score - Final'][44] = 23
#df_scorecard['Score - Final'][68] = 3
df_scorecard['Score - Final'][35] = 82
df_scorecard

min_sum_score_prel = df_scorecard.groupby('Original feature name')['Score - Final'].min().sum()
max_sum_score_prel = df_scorecard.groupby('Original feature name')['Score - Final'].max().sum()
print(min_sum_score_prel, max_sum_score_prel)

#### Calculating Credit Score

inputs_test_with_ref_cat_w_intecept = inputs_test_with_ref_cat
inputs_test_with_ref_cat_w_intecept.head()

inputs_test_with_ref_cat_w_intecept.insert(0, 'Intercept', 1)
inputs_test_with_ref_cat_w_intecept.head()

inputs_test_with_ref_cat_w_intecept.shape

inputs_test_with_ref_cat_w_intecept = inputs_test_with_ref_cat_w_intecept[df_scorecard['Feature name'].values]
inputs_test_with_ref_cat_w_intecept.head()

scorecard_scores = df_scorecard['Score - Final']

scorecard_scores.shape

scorecard_scores = scorecard_scores.values.reshape(102,1)

scorecard_scores.shape

y_scores = inputs_test_with_ref_cat_w_intecept.dot(scorecard_scores)
y_scores.head()

#### From Credit Score to PD 

sum_coef_from_score = ((y_scores - min_score)/(max_score - min_score)) * (max_sum_coef - min_sum_coef) + min_sum_coef

y_hat_test_proba_from_score = np.exp(sum_coef_from_score)/(1 + np.exp(sum_coef_from_score))
y_hat_test_proba_from_score.head()

y_hat_test_proba[0:5]

#### Setting a cut-off rate to decide if the application of loan to be approved or not

tr = 0.9
df_actual_predicted_probs['y_hat_test'] = np.where(df_actual_predicted_probs['y_hat_test_proba'] > tr, 1,0)

pd.crosstab(df_actual_predicted_probs['loan_data_targets_test'], df_actual_predicted_probs['y_hat_test'], rownames = ['Actuals'], colnames= ['Predicted'])

pd.crosstab(df_actual_predicted_probs['loan_data_targets_test'], df_actual_predicted_probs['y_hat_test'],
            rownames = ['Actuals'], colnames= ['Predicted']) / df_actual_predicted_probs.shape[0]

(pd.crosstab(df_actual_predicted_probs['loan_data_targets_test'], df_actual_predicted_probs['y_hat_test'],
            rownames = ['Actuals'], colnames= ['Predicted']) / df_actual_predicted_probs.shape[0]).iloc[0,0] + (pd.crosstab(df_actual_predicted_probs['loan_data_targets_test'], df_actual_predicted_probs['y_hat_test'],
            rownames = ['Actuals'], colnames= ['Predicted']) / df_actual_predicted_probs.shape[0]).iloc[1,1]

#try with tr = 0.9
#accuracy falls, but predicting number of people that can default increases which is good
#we need ROC curve to set appropriate tr
from sklearn.metrics import roc_curve, roc_auc_score

roc_curve(df_actual_predicted_probs['loan_data_targets_test'], df_actual_predicted_probs['y_hat_test_proba'])
#returns 3 arrays - False positive rate, True positive rate, threshold

fpr, tpr, threshold =  roc_curve(df_actual_predicted_probs['loan_data_targets_test'], df_actual_predicted_probs['y_hat_test_proba'])

import matplotlib.pyplot as plt
import seaborn as sns
sns.set()

plt.plot(fpr, tpr)
plt.plot(fpr, fpr, linestyle = '--', color = 'k')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ROC curve')

threshold

threshold.shape

df_cutoffs = pd.concat([pd.DataFrame(threshold), pd.DataFrame(fpr), pd.DataFrame(tpr)], axis = 1)

df_cutoffs.columns = ['threshold', 'fpr', 'tpr']

df_cutoffs.head()

# If 1st threshold is greater than 1 
df_cutoffs['threshold'][0]= 1 - 1/np.power(10,16) 

df_cutoffs['Score'] = ((np.log(df_cutoffs['threshold']/(1-df_cutoffs['threshold'])) - min_sum_coef) * ((max_score-min_score)/(max_sum_coef - min_sum_coef)) + min_score).round()

df_cutoffs.head()

df_cutoffs['Score'][0] = max_score

df_cutoffs.head()

df_cutoffs.tail()

def n_approved(p):
    return np.where(df_actual_predicted_probs['y_hat_test_proba']>=p, 1, 0).sum()

df_cutoffs['N Approved'] = df_cutoffs['threshold'].apply(n_approved)
df_cutoffs['N Rejected'] = df_actual_predicted_probs['y_hat_test_proba'].shape[0] - df_cutoffs['N Approved']
df_cutoffs['Approval Rate'] = df_cutoffs['N Approved'] / df_actual_predicted_probs['y_hat_test_proba'].shape[0]
df_cutoffs['Rejection Rate'] = 1- df_cutoffs['Approval Rate']

df_cutoffs.head()

df_cutoffs.tail()

#If highest level of PoD is 10% or the lowest possible probability of being good is 90%, 
#then at this cut-off, then the Approval rate would be 53.66% and rejection rate would be 46.34%
df_cutoffs.iloc[5000:6200, ] #index 5325

#If PoD = 5%
df_cutoffs.iloc[1000:2000, ]
#Approval rate = 20.94% and Rejection Rate = 79.06%, index = 1178

inputs_train_with_ref_cat.to_csv('C:/Users/vivek.goel/Downloads/Credit_Risk_Modelling_Udemy_course/inputs_train_with_ref_cat.csv')

df_scorecard.to_csv('C:/Users/vivek.goel/Downloads/Credit_Risk_Modelling_Udemy_course/df_scorecard.csv')
